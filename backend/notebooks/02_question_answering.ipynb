{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Demo\n",
    "\n",
    "This notebook demonstrates the multi-agent RAG pipeline:\n",
    "1. Intent classification (Router Agent)\n",
    "2. Hybrid retrieval (Retriever Agent)\n",
    "3. Cross-encoder reranking\n",
    "4. Answer generation with citations (Reasoning Agent)\n",
    "5. Confidence scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import django\n",
    "\n",
    "# Add backend to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')\n",
    "django.setup()\n",
    "\n",
    "print(\"Django setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.services import RAGOrchestrator\n",
    "from documents.models import Document, DocumentChunk\n",
    "from chat.models import ChatSession, ChatMessage\n",
    "\n",
    "# Check available documents\n",
    "documents = Document.objects.filter(status='READY')\n",
    "print(f\"Available documents: {documents.count()}\")\n",
    "for doc in documents:\n",
    "    print(f\"  - {doc.title} ({doc.num_chunks} chunks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize RAG Orchestrator\n",
    "\n",
    "The RAG Orchestrator manages the multi-agent workflow using LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the orchestrator\n",
    "orchestrator = RAGOrchestrator()\n",
    "\n",
    "print(\"RAG Orchestrator initialized!\")\n",
    "print(f\"Top-K retrieval: {orchestrator.top_k}\")\n",
    "print(f\"Graph nodes: {list(orchestrator.graph.nodes.keys()) if hasattr(orchestrator.graph, 'nodes') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Question Answering\n",
    "\n",
    "Let's ask a simple question about the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "query = \"What is machine learning?\"\n",
    "\n",
    "result = orchestrator.process_query(query)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nIntent: {result['metadata'].get('intent', 'N/A')}\")\n",
    "print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "print(f\"\\nConfidence: {result['metadata'].get('confidence_score', 'N/A'):.2%}\")\n",
    "\n",
    "if result['citations']:\n",
    "    print(f\"\\nCitations ({len(result['citations'])}):\\n\")\n",
    "    for i, citation in enumerate(result['citations']):\n",
    "        print(f\"  [{i+1}] {citation['document_title']}\")\n",
    "        print(f\"      Page: {citation.get('page', 'N/A')}\")\n",
    "        print(f\"      Snippet: {citation['snippet'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Complex Question with Multiple Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex question\n",
    "query = \"What are the different types of machine learning and their applications?\"\n",
    "\n",
    "result = orchestrator.process_query(query)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "print(f\"\\nConfidence: {result['metadata'].get('confidence_score', 'N/A'):.2%}\")\n",
    "print(f\"Chunks retrieved: {result['metadata'].get('num_retrieved', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Question with No Relevant Context\n",
    "\n",
    "Let's see how the system handles questions outside the document scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question outside document scope\n",
    "query = \"What is the weather forecast for tomorrow?\"\n",
    "\n",
    "result = orchestrator.process_query(query)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "print(f\"\\nConfidence: {result['metadata'].get('confidence_score', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conversation with History\n",
    "\n",
    "The system can maintain conversation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First question\n",
    "chat_history = []\n",
    "\n",
    "query1 = \"What is supervised learning?\"\n",
    "result1 = orchestrator.process_query(query1, chat_history)\n",
    "\n",
    "print(\"Q1:\", query1)\n",
    "print(\"A1:\", result1['answer'][:300] + \"...\")\n",
    "\n",
    "# Update history\n",
    "chat_history.append({\"role\": \"user\", \"content\": query1})\n",
    "chat_history.append({\"role\": \"assistant\", \"content\": result1['answer']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up question\n",
    "query2 = \"How does it differ from unsupervised learning?\"\n",
    "result2 = orchestrator.process_query(query2, chat_history)\n",
    "\n",
    "print(\"Q2:\", query2)\n",
    "print(\"A2:\", result2['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Persian Language Query\n",
    "\n",
    "The system supports multilingual queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persian query\n",
    "query = \"ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ú†ÛŒØ³ØªØŸ\"\n",
    "\n",
    "result = orchestrator.process_query(query)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nIntent: {result['metadata'].get('intent', 'N/A')}\")\n",
    "print(f\"\\nAnswer:\\n{result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Examine Retrieval Details\n",
    "\n",
    "Let's look at the retrieval process in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is deep learning?\"\n",
    "\n",
    "result = orchestrator.process_query(query)\n",
    "\n",
    "print(\"Retrieval Statistics:\")\n",
    "print(f\"  - Intent: {result['metadata'].get('intent')}\")\n",
    "print(f\"  - Initial retrieved: {result['metadata'].get('num_initial_retrieved', 'N/A')}\")\n",
    "print(f\"  - After reranking: {result['metadata'].get('num_after_rerank', 'N/A')}\")\n",
    "print(f\"  - Reranking applied: {result['metadata'].get('reranking_applied', 'N/A')}\")\n",
    "print(f\"  - Average relevance: {result['metadata'].get('avg_relevance', 'N/A')}\")\n",
    "print(f\"  - Confidence score: {result['metadata'].get('confidence_score', 'N/A'):.2%}\")\n",
    "\n",
    "print(\"\\nCitation relevance scores:\")\n",
    "for i, citation in enumerate(result['citations']):\n",
    "    score = citation.get('relevance_score', 'N/A')\n",
    "    if isinstance(score, float):\n",
    "        print(f\"  [{i+1}] {citation['document_title']}: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"  [{i+1}] {citation['document_title']}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Store Conversation in Database\n",
    "\n",
    "Let's save the conversation to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat session\n",
    "session = ChatSession.objects.create(title=\"Demo QA Session\")\n",
    "\n",
    "# Store conversation\n",
    "queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"What are its applications?\",\n",
    "    \"How is deep learning different?\"\n",
    "]\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "for query in queries:\n",
    "    # Store user message\n",
    "    user_msg = ChatMessage.objects.create(\n",
    "        session=session,\n",
    "        role=ChatMessage.Role.USER,\n",
    "        content=query\n",
    "    )\n",
    "    \n",
    "    # Get answer\n",
    "    result = orchestrator.process_query(query, chat_history)\n",
    "    \n",
    "    # Store assistant message\n",
    "    assistant_msg = ChatMessage.objects.create(\n",
    "        session=session,\n",
    "        role=ChatMessage.Role.ASSISTANT,\n",
    "        content=result['answer'],\n",
    "        metadata={\n",
    "            'citations': result['citations'],\n",
    "            'confidence_score': result['metadata'].get('confidence_score', 0)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Update history\n",
    "    chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": result['answer']})\n",
    "    \n",
    "    print(f\"Stored Q&A: {query[:50]}...\")\n",
    "\n",
    "print(f\"\\nSession ID: {session.id}\")\n",
    "print(f\"Total messages: {session.messages.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve conversation\n",
    "messages = ChatMessage.objects.filter(session=session).order_by('created_at')\n",
    "\n",
    "print(f\"Conversation in session {session.id}:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for msg in messages:\n",
    "    role = \"ðŸ‘¤ User\" if msg.role == 'user' else \"ðŸ¤– Assistant\"\n",
    "    print(f\"\\n{role}:\")\n",
    "    print(msg.content[:200] + \"...\" if len(msg.content) > 200 else msg.content)\n",
    "    \n",
    "    if msg.role == 'assistant' and msg.metadata.get('confidence_score'):\n",
    "        print(f\"\\n  [Confidence: {msg.metadata['confidence_score']:.2%}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Intent Classification**: Router Agent classifies queries into RAG_QUERY, SUMMARIZE, TRANSLATE, or CHECKLIST\n",
    "2. **Hybrid Retrieval**: Combines BM25 keyword search with vector similarity search\n",
    "3. **Cross-Encoder Reranking**: Uses LLM to rerank retrieved chunks for better relevance\n",
    "4. **Answer Generation**: Reasoning Agent generates grounded answers with citations\n",
    "5. **Confidence Scoring**: Each answer includes a confidence score based on retrieval quality and LLM confidence\n",
    "6. **Conversation History**: Maintains context across multiple questions\n",
    "7. **Multilingual Support**: Works with both English and Persian queries\n",
    "8. **Database Storage**: Conversations are persisted in PostgreSQL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
