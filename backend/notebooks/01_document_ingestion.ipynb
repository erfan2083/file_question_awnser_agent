{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Ingestion and Indexing Demo\n",
    "\n",
    "This notebook demonstrates the document ingestion pipeline:\n",
    "1. Uploading documents (PDF, DOCX, TXT, images)\n",
    "2. Text extraction\n",
    "3. Chunking with overlap\n",
    "4. Embedding generation\n",
    "5. Storage in PostgreSQL with pgvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up Django and import necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import django\n",
    "\n",
    "# Add backend to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')\n",
    "django.setup()\n",
    "\n",
    "print(\"Django setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from documents.models import Document, DocumentChunk\n",
    "from documents.services import DocumentProcessor\n",
    "from django.core.files.uploadedfile import SimpleUploadedFile\n",
    "import google.generativeai as genai\n",
    "from django.conf import settings\n",
    "\n",
    "print(f\"Gemini Model: {settings.GEMINI_MODEL}\")\n",
    "print(f\"Embedding Model: {settings.GEMINI_EMBEDDING_MODEL}\")\n",
    "print(f\"Chunk Size: {settings.CHUNK_SIZE}\")\n",
    "print(f\"Chunk Overlap: {settings.CHUNK_OVERLAP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document Upload\n",
    "\n",
    "Let's create a sample document and upload it to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample text document\n",
    "sample_text = \"\"\"\n",
    "Introduction to Machine Learning\n",
    "\n",
    "Machine learning is a subset of artificial intelligence (AI) that provides systems \n",
    "the ability to automatically learn and improve from experience without being \n",
    "explicitly programmed. Machine learning focuses on the development of computer \n",
    "programs that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning\n",
    "\n",
    "1. Supervised Learning: The algorithm learns from labeled training data and makes \n",
    "predictions based on that data. Examples include classification and regression tasks.\n",
    "\n",
    "2. Unsupervised Learning: The algorithm learns from unlabeled data and finds hidden \n",
    "patterns or intrinsic structures. Examples include clustering and dimensionality reduction.\n",
    "\n",
    "3. Reinforcement Learning: The algorithm learns by interacting with an environment \n",
    "and receiving rewards or penalties for actions taken.\n",
    "\n",
    "Applications of Machine Learning\n",
    "\n",
    "Machine learning has numerous applications across various industries:\n",
    "- Healthcare: Disease prediction, medical imaging analysis\n",
    "- Finance: Fraud detection, algorithmic trading\n",
    "- Technology: Speech recognition, recommendation systems\n",
    "- Transportation: Autonomous vehicles, route optimization\n",
    "\n",
    "Deep Learning\n",
    "\n",
    "Deep learning is a subset of machine learning that uses neural networks with \n",
    "multiple layers (deep neural networks). It has achieved remarkable success in \n",
    "tasks like image recognition, natural language processing, and speech recognition.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Sample text length: {len(sample_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document record\n",
    "document = Document.objects.create(\n",
    "    title=\"Introduction to Machine Learning\",\n",
    "    file_type=\"txt\",\n",
    "    file_size=len(sample_text),\n",
    "    language=\"en\",\n",
    "    status=Document.Status.UPLOADED\n",
    ")\n",
    "\n",
    "print(f\"Created document: {document}\")\n",
    "print(f\"Document ID: {document.id}\")\n",
    "print(f\"Status: {document.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Extraction and Chunking\n",
    "\n",
    "Now let's see how text chunking works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize text splitter with same settings as DocumentProcessor\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=settings.CHUNK_SIZE,\n",
    "    chunk_overlap=settings.CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split text into chunks\n",
    "chunks = text_splitter.split_text(sample_text)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i} (length: {len(chunk)} chars):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding Generation\n",
    "\n",
    "Let's generate embeddings for each chunk using Google Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini\n",
    "genai.configure(api_key=settings.GOOGLE_API_KEY, transport='rest')\n",
    "\n",
    "# Generate embedding for first chunk as example\n",
    "result = genai.embed_content(\n",
    "    model=settings.GEMINI_EMBEDDING_MODEL,\n",
    "    content=chunks[0],\n",
    "    task_type=\"retrieval_document\"\n",
    ")\n",
    "\n",
    "embedding = result['embedding']\n",
    "print(f\"Embedding dimensions: {len(embedding)}\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")\n",
    "print(f\"Embedding type: {type(embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Store Chunks with Embeddings\n",
    "\n",
    "Now let's store all chunks with their embeddings in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings and store chunks\n",
    "chunk_objects = []\n",
    "\n",
    "for idx, chunk_text in enumerate(chunks):\n",
    "    # Generate embedding\n",
    "    result = genai.embed_content(\n",
    "        model=settings.GEMINI_EMBEDDING_MODEL,\n",
    "        content=chunk_text,\n",
    "        task_type=\"retrieval_document\"\n",
    "    )\n",
    "    embedding = result['embedding']\n",
    "    \n",
    "    # Create chunk object\n",
    "    chunk = DocumentChunk(\n",
    "        document=document,\n",
    "        index=idx,\n",
    "        text=chunk_text,\n",
    "        embedding=embedding,\n",
    "        char_count=len(chunk_text),\n",
    "        token_count=len(chunk_text.split())\n",
    "    )\n",
    "    chunk_objects.append(chunk)\n",
    "    print(f\"Created chunk {idx}: {len(chunk_text)} chars, {len(embedding)} dim embedding\")\n",
    "\n",
    "# Bulk create chunks\n",
    "DocumentChunk.objects.bulk_create(chunk_objects)\n",
    "print(f\"\\nStored {len(chunk_objects)} chunks in database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update document status\n",
    "document.status = Document.Status.READY\n",
    "document.num_chunks = len(chunk_objects)\n",
    "document.num_pages = 1\n",
    "document.save()\n",
    "\n",
    "print(f\"Document status: {document.status}\")\n",
    "print(f\"Number of chunks: {document.num_chunks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Storage\n",
    "\n",
    "Let's verify that everything was stored correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query stored chunks\n",
    "stored_chunks = DocumentChunk.objects.filter(document=document).order_by('index')\n",
    "\n",
    "print(f\"Retrieved {stored_chunks.count()} chunks from database\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "for chunk in stored_chunks:\n",
    "    print(f\"\\nChunk {chunk.index}:\")\n",
    "    print(f\"  - Characters: {chunk.char_count}\")\n",
    "    print(f\"  - Tokens: {chunk.token_count}\")\n",
    "    print(f\"  - Embedding dimensions: {len(chunk.embedding)}\")\n",
    "    print(f\"  - Text preview: {chunk.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vector Similarity Search\n",
    "\n",
    "Let's test vector similarity search on our stored chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Test query\n",
    "query = \"What are the applications of machine learning?\"\n",
    "\n",
    "# Generate query embedding\n",
    "query_result = genai.embed_content(\n",
    "    model=settings.GEMINI_EMBEDDING_MODEL,\n",
    "    content=query,\n",
    "    task_type=\"retrieval_query\"\n",
    ")\n",
    "query_embedding = np.array(query_result['embedding'])\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Query embedding dimensions: {len(query_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity with all chunks\n",
    "similarities = []\n",
    "\n",
    "for chunk in stored_chunks:\n",
    "    chunk_embedding = np.array(chunk.embedding)\n",
    "    \n",
    "    # Cosine similarity\n",
    "    similarity = np.dot(query_embedding, chunk_embedding) / (\n",
    "        np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)\n",
    "    )\n",
    "    similarities.append((chunk, similarity))\n",
    "\n",
    "# Sort by similarity (descending)\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Search Results (sorted by similarity):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for chunk, sim in similarities:\n",
    "    print(f\"\\nChunk {chunk.index} (similarity: {sim:.4f}):\")\n",
    "    print(f\"  {chunk.text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup\n",
    "\n",
    "Let's clean up the demo document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally delete the demo document\n",
    "# document.delete()\n",
    "# print(\"Demo document deleted\")\n",
    "\n",
    "print(\"Demo complete! Document remains in database for further testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated:\n",
    "\n",
    "1. **Document Upload**: Creating a document record in Django\n",
    "2. **Text Chunking**: Using LangChain's RecursiveCharacterTextSplitter with overlap\n",
    "3. **Embedding Generation**: Using Google Gemini's text-embedding-004 model\n",
    "4. **Storage**: Storing chunks with 768-dimensional embeddings in PostgreSQL with pgvector\n",
    "5. **Vector Search**: Performing cosine similarity search on stored embeddings\n",
    "\n",
    "The system supports:\n",
    "- PDF, DOCX, TXT, JPG, JPEG, PNG file formats\n",
    "- Configurable chunk size and overlap\n",
    "- Hybrid retrieval (BM25 + vector search)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
